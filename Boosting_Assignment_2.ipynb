{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ccf2f-19f7-41de-b0a6-ad12ae80af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 1\n",
    "# Ans -- Gradient Boosting Regression, often referred to as simply Gradient Boosting or GBM (Gradient Boosting Machine), is a powerful machine learning technique used for regression tasks. It belongs to the family of ensemble learning methods, where multiple models (typically decision trees) are combined to create a stronger, more accurate predictive model.\n",
    "\n",
    "The key idea behind Gradient Boosting Regression is to build weak learners (usually shallow decision trees) sequentially, with each one focused on correcting the errors made by its predecessors. Here's how it works:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The algorithm starts with an initial prediction, which is usually the mean value of the target variable for regression tasks.\n",
    "\n",
    "2. **Calculate Residuals**:\n",
    "   - The first model (weak learner) is trained to predict the residuals (the differences between the actual and initial predicted values) of the target variable.\n",
    "\n",
    "3. **Update Predictions**:\n",
    "   - The predictions of the first model are added to the initial predictions to create an updated set of predictions.\n",
    "\n",
    "4. **Repeat the Process**:\n",
    "   - The residuals are calculated again, and a new model is trained on the residuals. This model focuses on correcting the errors made by the previous models.\n",
    "\n",
    "5. **Combine Predictions**:\n",
    "   - The predictions of all weak learners are combined to form the final prediction.\n",
    "\n",
    "6. **Final Model**:\n",
    "   - The final model is a weighted sum of the predictions from all the weak learners.\n",
    "\n",
    "The term \"Gradient\" in Gradient Boosting refers to the fact that the algorithm uses gradient descent to minimize a loss function. In regression tasks, the loss function is typically the mean squared error (MSE), which measures the average of the squared differences between predicted and actual values.\n",
    "\n",
    "The process continues for a predefined number of iterations (or until a certain level of accuracy is reached). Each new model corrects the errors of its predecessors, leading to a strong learner that is capable of accurately predicting the target variable.\n",
    "\n",
    "Popular implementations of Gradient Boosting for regression include algorithms like XGBoost, LightGBM, and CatBoost. These implementations often incorporate additional optimizations and features to enhance performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8dba8f0-417e-402a-a7c9-daa56e5b3fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 8.4401\n",
      "R-squared: -3.6745\n"
     ]
    }
   ],
   "source": [
    "# Ques 2\n",
    "# Ans -- \n",
    "import numpy as np\n",
    "\n",
    "# Define a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([2, 3, 4, 3, 5, 6])\n",
    "\n",
    "# Define the number of weak learners (trees)\n",
    "num_learners = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions with the mean of y\n",
    "predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "# Implement the gradient boosting algorithm\n",
    "for _ in range(num_learners):\n",
    "    # Calculate residuals\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Train a decision tree on the residuals (for simplicity, we'll use a simple mean value)\n",
    "    tree_prediction = np.mean(residuals)\n",
    "    \n",
    "    # Update predictions\n",
    "    predictions = learning_rate * tree_prediction +1\n",
    "\n",
    "# Evaluate the model\n",
    "mse = np.mean((y - predictions) ** 2)\n",
    "ssr = np.sum((y - predictions) ** 2)\n",
    "sst = np.sum((y - np.mean(y)) ** 2)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc61d5cb-e1c1-4cf5-903c-4072ae5103b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "\n",
      "Mean Squared Error (MSE): 0.7066\n",
      "R-squared: 0.6086\n"
     ]
    }
   ],
   "source": [
    "# Ques 3\n",
    "# Ans -- \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor model\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "mse = np.mean((y - y_pred) ** 2)\n",
    "r_squared = best_model.score(X, y)\n",
    "\n",
    "print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd36026-006b-4570-9887-86dfd1e21403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 4 \n",
    "# Ans - A weak learner in the context of Gradient Boosting is a base model that performs slightly better than random chance on a given learning task. Unlike strong learners, which can model complex relationships in the data, weak learners are deliberately kept simple and have limited predictive power.\n",
    "\n",
    "Common examples of weak learners include:\n",
    "\n",
    "1. **Decision Stumps**: These are decision trees with only one split, meaning they make predictions based on a single feature. They are very simple models and are often used as weak learners in boosting algorithms.\n",
    "\n",
    "2. **Shallow Decision Trees**: Decision trees with a limited depth (e.g., 1 to 3 levels) are commonly used as weak learners. These trees are not able to capture complex relationships in the data, making them suitable candidates.\n",
    "\n",
    "3. **Linear Models**: Simple linear models like linear regression or logistic regression can also be used as weak learners, especially in gradient boosting algorithms like Linear Boosting.\n",
    "\n",
    "4. **Polynomial Models**: Models that capture low-degree polynomial relationships can also serve as weak learners.\n",
    "\n",
    "5. **Gaussian Mixture Models**: These probabilistic models can be used for density estimation tasks.\n",
    "\n",
    "The key characteristic of a weak learner is that it has some predictive ability above random chance, but it may struggle to capture more complex patterns in the data. This is intentional in the context of boosting algorithms, as each weak learner's primary role is to focus on the mistakes made by its predecessors.\n",
    "\n",
    "The strength of boosting algorithms lies in their ability to sequentially train and combine a series of weak learners, each one improving upon the mistakes of the previous models. By doing so, the ensemble gradually builds up a highly accurate predictive model.\n",
    "\n",
    "It's worth noting that while individual weak learners may not be very powerful, their collective wisdom, when combined in an ensemble, can lead to a strong learner capable of capturing intricate relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3ba70-cf03-433c-90ce-3f5e60953d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 5 \n",
    "# Ans -- The intuition behind the Gradient Boosting algorithm can be understood through the metaphor of a team of \"experts\" working together to solve a problem. Each expert (weak learner) has a specific area of expertise, but none of them is perfect. They work together sequentially, with each expert focused on correcting the mistakes made by the previous ones.\n",
    "\n",
    "Here's a more detailed breakdown of the intuition behind Gradient Boosting:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The algorithm starts with an initial prediction, often just the mean value of the target variable for regression tasks. This represents the initial consensus opinion of the experts.\n",
    "\n",
    "2. **First Expert (Weak Learner)**:\n",
    "   - The first expert examines the data and identifies the errors (residuals) in the initial prediction. This expert is not perfect, but it specializes in a certain type of mistake.\n",
    "\n",
    "3. **Updating Predictions**:\n",
    "   - The first expert's correction is added to the initial prediction. This leads to a new prediction, which is a more accurate estimate of the target variable.\n",
    "\n",
    "4. **Second Expert (Weak Learner)**:\n",
    "   - The second expert examines the residuals from the updated prediction and identifies a different type of error. This expert specializes in a different aspect of the problem.\n",
    "\n",
    "5. **Iterative Process**:\n",
    "   - The process continues, with each new expert focusing on the remaining errors and providing further refinements to the prediction.\n",
    "\n",
    "6. **Combining Expert Opinions**:\n",
    "   - As more experts join the team, their individual contributions are combined to form a final prediction. Each expert has a say in the final decision, but their influence is weighted based on their expertise.\n",
    "\n",
    "The key intuition behind Gradient Boosting is that each expert (weak learner) specializes in a specific aspect of the problem. They are not expected to be perfect, but they should be slightly better than random chance. By combining their efforts and allowing them to correct each other's mistakes, the ensemble (team of experts) is able to arrive at a highly accurate prediction.\n",
    "\n",
    "The \"gradient\" in Gradient Boosting refers to the fact that, in each step, the algorithm tries to find the direction (gradient) in which it can improve the prediction the most. This is done by training each weak learner to predict the residual errors of the previous models. This iterative process of minimizing the loss function ultimately leads to a strong learner with high predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ba2f8-578a-4f08-bedc-ba757ec28b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 6 \n",
    "# Ans -- The Gradient Boosting algorithm builds an ensemble of weak learners (usually decision trees) in a sequential manner. Each weak learner is trained to correct the errors made by its predecessors. Here's how the algorithm constructs the ensemble:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The algorithm starts with an initial prediction, often the mean value of the target variable for regression tasks. This is used as the starting point for the ensemble.\n",
    "\n",
    "2. **Calculate Residuals**:\n",
    "   - The first weak learner is trained on the data, and it attempts to predict the residuals (errors) of the initial prediction.\n",
    "\n",
    "3. **Update Predictions**:\n",
    "   - The predictions of the first weak learner are added to the initial prediction to form an updated set of predictions.\n",
    "\n",
    "4. **Next Weak Learner**:\n",
    "   - Another weak learner is introduced to the ensemble. It's trained to predict the residuals that were not captured by the first model. This new model focuses on correcting the errors made by the initial prediction and the first model.\n",
    "\n",
    "5. **Update Predictions Again**:\n",
    "   - The predictions of the second weak learner are added to the updated predictions from the first model.\n",
    "\n",
    "6. **Iterative Process**:\n",
    "   - The process continues for a predefined number of iterations or until a certain level of accuracy is reached. In each iteration, a new weak learner is introduced, and it corrects the remaining errors from the previous models.\n",
    "\n",
    "7. **Combine Predictions**:\n",
    "   - The final prediction is a weighted sum of the predictions from all the weak learners. Models that performed better are given more weight.\n",
    "\n",
    "The key idea is that each new weak learner is specialized in correcting the errors made by its predecessors. By doing so, the ensemble gradually improves its predictive accuracy.\n",
    "\n",
    "The process of constructing the ensemble involves finding the optimal parameters for each weak learner, such as the split points in decision trees or coefficients in linear models. This is done using gradient descent to minimize a loss function, typically the mean squared error for regression tasks.\n",
    "\n",
    "The final result is a strong learner that effectively combines the predictions of the weak models. This ensemble model is capable of making highly accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909152ed-3977-41ab-adf9-60996a90e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 7 \n",
    "# Ans-- Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the following key steps:\n",
    "\n",
    "1. **Loss Function and Optimization**:\n",
    "   - Define a loss function that measures the difference between the predicted and actual values. For regression tasks, the mean squared error (MSE) is a common choice. The goal is to minimize this loss function.\n",
    "\n",
    "2. **Initial Prediction**:\n",
    "   - Start with an initial prediction, often the mean value of the target variable. This serves as the base prediction for the ensemble.\n",
    "\n",
    "3. **Calculate Residuals**:\n",
    "   - Calculate the residuals, which are the differences between the actual and predicted values. These represent the errors made by the current prediction.\n",
    "\n",
    "4. **Train Weak Learner on Residuals**:\n",
    "   - Train a weak learner (e.g., a decision tree) on the residuals. The goal is to build a model that can predict the remaining errors.\n",
    "\n",
    "5. **Update Predictions**:\n",
    "   - Combine the predictions of the current weak learner with the previous predictions. This gives an updated set of predictions.\n",
    "\n",
    "6. **Calculate New Residuals**:\n",
    "   - Calculate the residuals again, using the updated predictions. These residuals represent the remaining errors after the current weak learner's correction.\n",
    "\n",
    "7. **Train Next Weak Learner on New Residuals**:\n",
    "   - Introduce a new weak learner to the ensemble. Train it on the new residuals, aiming to further correct the errors.\n",
    "\n",
    "8. **Iterative Process**:\n",
    "   - Repeat steps 5 to 7 for a predefined number of iterations. Each new weak learner focuses on the remaining errors from the previous models.\n",
    "\n",
    "9. **Combine Predictions for Final Model**:\n",
    "   - The final prediction is obtained by combining the predictions of all the weak learners. Models that performed better are given more weight.\n",
    "\n",
    "10. **Regularization (Optional)**:\n",
    "    - Optionally, apply regularization techniques to prevent overfitting. This may include constraints on the complexity of the weak learners or the addition of penalties to the loss function.\n",
    "\n",
    "11. **Learning Rate (Optional)**:\n",
    "    - Introduce a learning rate parameter to control the contribution of each weak learner. A lower learning rate means that each model's contribution is smaller, which can improve the stability of the training process.\n",
    "\n",
    "12. **Final Model Evaluation**:\n",
    "    - Evaluate the final ensemble model on a validation or test set using appropriate metrics (e.g., MSE, R-squared).\n",
    "\n",
    "These steps collectively form the mathematical intuition behind Gradient Boosting. The algorithm's effectiveness lies in its ability to iteratively correct the errors made by the previous models, gradually improving the predictive accuracy of the ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
